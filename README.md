# TraumaICDBERT: A Natural Language Processing Algorithm to Automate Injury ICD-10 Diagnosis Code Extraction from Free Text 

A BERT-based deep learning NLP algorithm to extract ICD-10 codes from unstructured tertiary survey notes of trauma patients.


<img src="https://github.com/asivura/trauma-icd/blob/main/figures/Trauma%20ICD%20Overview.png" width="500">


# Usage

1. Manually collect training data (ICD10 codes) from each EMR. See example here: [Google sheet](https://docs.google.com/spreadsheets/d/19PKbWvzFohSQhzaMaz9lvfDuOqMZI8ZJM7aqzZ57Xeg/edit?usp=sharing)
2. Obtain a list of ICD-10 candidate codes from the official schema `notebooks/ohdsi-vocab.ipynb`
3. Create a list of training, test, and validation patient_ids using `notebooks/prepare-dataset.ipynb`
4. Prepare the input-label dataset using `notebooks/prepare-dataset.ipynb`
5. Generate ICD-10 code semantic similarity scores using `notebooks/gpt3-embeddings.ipynb`
6. Run the training script using a Google Colab notebook: `notebooks/train-colab.ipynb`, or using command line:

```
train.py --model_name=$model_name \
                 --data_dir=$DS_HOME \
                 --model_dir=$MODEL_HOME \
                 --experiment_name=$experiment_name \
                 --valid_labels=$valid_labels \
                 --is_evaluate=$evaluate_only \
                 --train_on_full=$train_on_full \
                 --num_train_epochs=$num_epochs \
                 --metric_for_best_model=$metric_for_best_model \
                 --learning_rate=$learning_rate \
                 --warmup_steps=$warmup_steps \
                 --per_device_train_batch_size=$per_device_train_batch_size \
                 --per_device_eval_batch_size=$per_device_eval_batch_size

arguments:

    --model_name: name of the Huggingface model to be trained
    --data_dir: directory where the data is stored.
    --model_dir: directory where the model will be stored.
    --experiment_name: name of the experiment, can be an arbitrary name to identify results from runs.
    --valid_labels: the valid ICD-10 labels, e.g., 4-char, 4-char-top50
    --is_evaluate: boolean to indicate whether to evaluate the model only and not train
    --train_on_full: boolean to indicate whether to train on full dataset, and eval on test set.
    --num_train_epochs: number of epochs to train the model.
    --metric_for_best_model: metric to be used for determining the best model.
    --learning_rate: learning rate for the model.
    --warmup_steps: number of warmup steps for the model.
    --per_device_train_batch_size: batch size for training.
    --per_device_eval_batch_size: batch size for evaluation.

example argument values:

DS_HOME = 'PROJECT_ROOT_DIR/injury-icd-dataset'
CODE_HOME = 'PROJECT_ROOT_DIR/code'
MODEL_HOME = 'PROJECT_ROOT_DIR/models'
valid_labels = "4-char"
experiment_name = "non-sup-4-char-train-on-full"
model_name = "michiyasunaga/BioLinkBERT-large"
metric_for_best_model = "eval_f1_score_weighted"
num_epochs = 20
train_on_full = True
evaluate_only = False
learning_rate = 2e-5
warmup_steps = 5000
per_device_train_batch_size = 16
per_device_eval_batch_size = 32

```
7. The evaluation scripts for Amazon Web Services Comprehend Medical is available in the `/notebooks` directory.

# File Structure

```
./
    injury-icd-dataset/
        case.csv                        -- input text and patient_id, generated by `notebooks/prepare-dataset.ipynb`
        case-labels.csv                 -- each row contains the patient id and an ICD-10 label, generated by ...
        label-case-count.csv            -- the frequency of each 4-character ICD-10 code, generated by ...
        label-non-superficial-top10.txt -- list of top 10 non-superficial injury ICD-10 codes, generated by ...
        label-non-superficial-top50.txt -- list of top 50 non-superficial injury ICD-10 codes, generated by ...
        label-non-superficial.txt       -- list of all non-superficial injury ICD-10 codes, generated by ...
        train.txt                       -- contains training patient id on each line, generated by ...
        validation.txt                  -- contains validation patient id on each line, generated by ...
        train_and_validation.txt       -- contains training and validation patient id on each line, generated by ...
        test.txt                        -- contains test patient id on each line, generated by ...
        icd-name-davinci-001-simularity-scores.csv -- similarity scores between ICD-10 codes, generated by `notebooks/gpt3-embeddings`
    
    models/                             -- files under this directory will be automatically generated by train.py
        michiyasunaga/
            BioLinkBERT-base/
                encoded_ds/             -- encoded dataset, auto-generated using the corresponding HF tokenizer
                    train/
                        ...                 
                    test/
                        ...                 
                    reduced_validation/
                        ...                 
                    validation/
                        ...                 
                    dataset_dict.json
                EXPERIMENT_NAME_1/          -- automatically created by the script
                    checkpoint-12345/
                        config.json
                        pytorch_model.bin
                        trainer_state.json
                        ...
                    checkpoint-67890/
                        ...
                EXPERIMENT_NAME_2/          
                    ...
        ...                              -- other BERT models on HuggingFace can be used as well, e.g., PubMedBERT
    
    notebooks/
        evaluate-awscm.ipynb             -- evaluates AWSCM predictions, requires using AWSCM to process EMRs first
        prepare-awscm-for-evaluation.ipynb.ipynb -- uses AWSCM to process EMRs, requires AWSCM predictions
        gpt3-embeddings.ipynb            -- generates GPT3 embeddings for the dataset, to reduce validation time
        ohdsi-vocab.ipynb                -- generates a list of candidate ICD-10 codes for the dataset
        prepare-dataset.ipynb            -- generates the dataset using patient EMRs and ICD-10 codes for training
        train-colab.ipynb                -- runs the training script on Google Colab cloud GPU

    code/
        dataset.py      -- generates input-output pairs from dataset of patient EMR with ICD-10 code labels
        train.py        -- the main script to train, evaluate, and save the model
        evaluate.py     -- evaluation scripts for obtaining AUC, F1, precision@k, and recall@k scores
        model.py        -- for loading and saving the model checkpoint
    
```

# Results 

|               | Top 10 codes | Top 10 codes | Top 50 codes | Top 50 codes| All 170 codes | All 170 codes|
|---------------|:------------------------------------------------:|:-------------------------:|:------------------------------------------------:|:-------------------------:|:--------------------------------------:|:-------------------------:|
|               |                  TraumaICD- BERT                 | Amazon Comprehend Medical |                  TraumaICD- BERT                 | Amazon Comprehend Medical |             TraumaICD- BERT            | Amazon Comprehend Medical |
|   AUROC_micro  |                       98.6                       |            82.6           |                       96.0                       |            80.5           |                  95.7                  |            80.0           |
|   AUROC_macro  |                       98.1                       |            79.1           |                       92.8                       |            76.2           |                  90.0                  |            70.9           |
| AUROC_weighted |                       98.1                       |            82.4           |                       93.3                       |            79.4           |                  92.2                  |            77.4           |
|    F1_micro    |                       87.0                       |            59.6           |                       72.6                       |            43.4           |                  66.4                  |            32.2           |
|    F1_macro    |                       84.1                       |            53.8           |                       66.5                       |            36.8           |                  41.8                  |            18.8           |
|   F1_weighted  |                       87.1                       |            61.4           |                       72.0                       |            47.4           |                  65.6                  |            41.0           |
|  Precision@5  |                       36.6                       |            30.2           |                       46.6                       |            31.2           |                  47.7                  |            26.6           |
|  Precision@10 |                       18.1                       |            18.1           |                       27.9                       |            20.8           |                  29.3                  |            19.3           |
|  Precision@20 |                       18.1                       |            18.1           |                       15.2                       |            12.1           |                  16.3                  |            11.6           |
|    Recall@5   |                       99.4                       |            85.7           |                       83.4                       |            59.1           |                  75.2                  |            46.3           |
|   Recall@10   |                       100.0                      |           100.0           |                       93.0                       |            71.0           |                  85.1                  |            60.5           |
|   Recall@20   |                       100.0                      |           100.0           |                       98.4                       |            79.5           |                  91.1                  |            67.1           |

Comparative performance of TraumaICDBERT and Amazon Web Service Comprehend Medical for predicting 4-character injury ICD-10 diagnosis codes from free text (tertiary survey notes). TraumaICDBERT's performance exceeded or matched that of Amazon Web Service Comprehend Medical across all metrics.

# Training Details

The training task is formatted as follows: the input consists of one ICD-10 code definition (e.g., Multiple fractures of ribs) and the tertiary EMR (imaging report + tertiary impression), the output is a probability between 1 (positive ground truth example) and 0 (negative example). For each patient, we generate 10/50/170 such input-output pairs, where the label for positive example is 1, and negative example is 0. We fine-tune the model using a pre-trained BioLinkBERT on this classification task. 

During training, the model is trained on all the positive examples, as well as an equal number of negative examples. Since there are more negative examples than positive examples (a patient may have 4 injuries while there are 170 candidate codes), we randomly sample a subset of the negative examples at each epoch. 

During reduced_validation, the model is evaluated on all the positive examples and a subset of the negative examples (to improve computational efficiency, we select only the most challenging codes that have similar GPT-3 embeddings with the positive code). 

When the model is finished training (e.g., for 20 epochs), the model is evaluated in a final loop of all positive examples and all negative examples. For example, each EMR will be inferenced 170 times, where each inference corresponds to a candidate ICD-10 code. The model evaluation metrics is computed using this probability matrix.

The training time of BioLinkBERT-base for 20 epochs on a P100 GPU is less than a day.

# Hardware Requirements

The entire tokenized dataset (encoded_ds) must be able to fit onto the computer RAM. We used a workstation with 24GB of RAM when using a dataset of 3,500 EMRs, where each EMR has 170 input-output pair examples.

The recommended batch size for training BioLinkBERT-base on P100 GPU (16GB VRAM) is 16 for training and 32 for evaluation.

The recommended batch size for training BioLinkBERT-large on P100 GPU (16GB VRAM) is 6 for training and 12 for evaluation.


# Support
If you have any questions regarding this repository, please open a support ticket on Github, or write to yifuchen [a] stanford [.] edu for guidance.
