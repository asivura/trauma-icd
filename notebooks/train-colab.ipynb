{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmsGhOs4vAuA",
        "outputId": "437b2dd1-7d8a-4982-8ec8-6ddfd7aba893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jul 29 16:11:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "alLOMYe2UgeF",
        "outputId": "f14d2e57-fd83-4121-aa4a-07e57a9bd01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 365 kB 8.3 MB/s \n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 76.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.7 MB/s \n"
          ]
        }
      ],
      "source": [
        "!pip install datasets -q\n",
        "!pip install transformers -q -U\n",
        "!pip install wandb -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZItXZ1SMjK1",
        "outputId": "a0b774a8-0165-41ad-a431-40e21e97297e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "DS_HOME = '/content/gdrive/Shareddrives/PROJECT_ROOT_DIR/injury-icd-dataset'\n",
        "CODE_HOME = '/content/gdrive/Shareddrives/PROJECT_ROOT_DIR/code'\n",
        "MODEL_HOME = '/content/gdrive/Shareddrives/PROJECT_ROOT_DIR/models'\n",
        "TMP_DIR = '/content/tmp'\n",
        "\n",
        "if not os.path.exists(TMP_DIR):\n",
        "    os.mkdir(TMP_DIR)\n",
        "\n",
        "os.chdir(CODE_HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zeo0Kk5WV9L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "import sys, importlib\n",
        "import random\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVish44lc4r0",
        "outputId": "c0eb3e9d-f2be-4c7b-8e89-669b4a6de9f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The suggested batch size for train and eval of large-sized model on P100 GPU is 6 and 12\n"
          ]
        }
      ],
      "source": [
        "valid_labels = \"4-char\"                          #@param [\"4-char\", \"4-char-top50\", \"4-char-top10\", \"5-char\", \"4-and-5-char\"]\n",
        "experiment_name = \"non-sup-4-char-train-on-full\" #@param {type:\"string\",  allow-input: true} [\"non-sup\", \"non-sup-pretrain\", \"non-sup-tune-after-pretrain\", \"non-sup-4and5-char-train-on-full\", \"non-sup-train-on-full\"]\n",
        "model_name = \"michiyasunaga/BioLinkBERT-base\"         #@param [\"michiyasunaga/BioLinkBERT-base\", \"michiyasunaga/BioLinkBERT-large\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"]\n",
        "metric_for_best_model = \"eval_f1_score_weighted\"          #@param [\"eval_f1_score_macro\", \"eval_f1_score_micro\", \"eval_f1_score_weighted\", \"eval_auc_score_macro\", \"eval_auc_score_micro\", \"eval_auc_score_weighted\"]\n",
        "num_epochs = \"20\"                 #@param [6, 10, 20, 30]\n",
        "# Whether to train on both train and val dataset, and do final eval on holdout test set\n",
        "train_on_full = True            #@param {type:\"boolean\"} \n",
        "evaluate_only = True           #@param {type:\"boolean\"}\n",
        "learning_rate = \"0.00002\"        #@param [2e-5, 1e-5, 7e-6, 2e-6]\n",
        "warmup_steps =  5000            #@param [1000, 2000, 3000, 5000]\n",
        "per_device_train_batch_size = \"16\" #@param [16, 8, 6, 4]\n",
        "per_device_eval_batch_size = \"32\" #@param [32, 16, 12, 8]\n",
        "learning_rate = float(learning_rate)\n",
        "warmup_steps = int(warmup_steps)\n",
        "per_device_train_batch_size = int(per_device_train_batch_size)\n",
        "per_device_eval_batch_size = int(per_device_eval_batch_size)\n",
        "if \"base\" in model_name:\n",
        "  print(\"The suggested batch size for train and eval of base-sized model on P100 GPU is 16 and 32\")\n",
        "else:\n",
        "  print(\"The suggested batch size for train and eval of large-sized model on P100 GPU is 6 and 12\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptIEdaI6Wg9_",
        "outputId": "837b4e1d-9ca0-4160-c77e-cc0bc519972b"
      },
      "outputs": [],
      "source": [
        "!python train.py --model_name=$model_name \\\n",
        "                 --data_dir=$DS_HOME \\\n",
        "                 --model_dir=$MODEL_HOME \\\n",
        "                 --experiment_name=$experiment_name \\\n",
        "                 --valid_labels=$valid_labels \\\n",
        "                 --is_evaluate=$evaluate_only \\\n",
        "                 --train_on_full=$train_on_full \\\n",
        "                 --num_train_epochs=$num_epochs \\\n",
        "                 --metric_for_best_model=$metric_for_best_model \\\n",
        "                 --learning_rate=$learning_rate \\\n",
        "                 --warmup_steps=$warmup_steps \\\n",
        "                 --per_device_train_batch_size=$per_device_train_batch_size \\\n",
        "                 --per_device_eval_batch_size=$per_device_eval_batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTti4izCj-62"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
